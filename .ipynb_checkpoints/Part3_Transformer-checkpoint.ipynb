{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18sd8-wHGDR0"
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 3: Transformer\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiCHKxjfGLRu"
   },
   "source": [
    "In this part we will implement a variation of the attention mechanism named the 'sliding window attention'. Next, we will create a transformer encoder with the sliding-window attention implementation, and we will train the encoder for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0t2UQaeKCKxa",
    "outputId": "10e4f165-d0c0-4ac9-960a-6796ff3c1750"
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35B3L7USC5PE",
    "outputId": "5e05f218-468a-4e54-c9bb-a7357f0dfe09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "test = unittest.TestCase()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kRUk_GYNDd5"
   },
   "source": [
    "## Reminder: scaled dot product attention\n",
    "<a id=part3_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30P7CuIwNKy-"
   },
   "source": [
    "In class, you saw that the scaled dot product attention is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mat{B} &= \\frac{1}{\\sqrt{d}} \\mat{Q}\\mattr{K}  \\ \\in\\set{R}^{m\\times n} \\\\\n",
    "\\mat{A} &= softmax({\\mat{B}},{\\mathrm{dim}=1}), \\in\\set{R}^{m\\times n} \\\\\n",
    "\\mat{Y} &= \\mat{A}\\mat{V} \\ \\in\\set{R}^{m\\times d_v}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where `K`,`Q` and `V` for the self attention came as projections of the same input sequnce\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{q}_{i} &= \\mat{W}_{xq}\\vec{x}_{i} &\n",
    "\\vec{k}_{i} &= \\mat{W}_{xk}\\vec{x}_{i} &\n",
    "\\vec{v}_{i} &= \\mat{W}_{xv}\\vec{x}_{i} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If you feel the attention mechanism doesn't quite sit right, we recommend you go over lecture and tutorial notes before proceeding. \n",
    "\n",
    "We are now going to introduce a slight variation of the scaled dot product attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8CMV4VKG7YB"
   },
   "source": [
    "## Sliding window attention\n",
    "<a id=part3_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YphH4DuEHP1p"
   },
   "source": [
    "The scaled dot product attention computes the dot product between **every** pair of key and query vectors. Therefore, the computation complexity is $O(n^2)$ where $n$ is the sequence length.\n",
    "\n",
    "In order to obtain a computational complexity that grows linearly with the sequnce length, the authors of 'Longformer: The Long-Document Transformer https://arxiv.org/pdf/2004.05150.pdf' proposed the 'sliding window attention' which is a variation of the scaled dot product attention. \n",
    "\n",
    "In this variation, instead of computing the dot product for every pair of key and query vectors, the dot product is only computed for keys that are in a certain 'window' around the query vector. \n",
    "\n",
    "For example, if the keys and queries are embeddings of words in the sentence \"CS is more prestigious than EE\", and the window size is 2, then for the query corresponding to the word 'is' we will only compute a dot product with the keys that are at most ${window\\_size}\\over{2}$$ = $${2}\\over{2}$$=1$ to the left and to the right. Meaning the keys that correspond to the workds 'CS', 'is' and 'more'.\n",
    "\n",
    "Formally, the intermediate calculation of the normalized dot product can be written as: \n",
    "\n",
    "$$\n",
    "\\mathrm{b}(q, k, w) \n",
    "=\n",
    "\\begin{cases}\n",
    "    q⋅k^T\\over{\\sqrt{d_k}} & \\mathrm{if} \\;d(q,k) ≤ {{w}\\over{2}} \\\\\n",
    "    -\\infty & \\mathrm{otherwise}\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "Where $b(\\cdot,\\cdot,\\cdot)$ is the intermediate result function (used to construct a matrix $\\mat{B}$ on which we perform the softmax), $q$ is the query vector, $k$ is the key vector, $w$ is the sliding window size, and $d(\\cdot,\\cdot)$ is the distance function between the positions of the tokens corresponding to the key and query vectors.\n",
    "\n",
    "**Note**: The distance function $d(\\cdot,\\cdot)$ is **Not** cyclical. Meaning that that in the example above when searching for the words at distance 1 from the word 'CS', we **don't** return cyclically from the right and count the word EE.\n",
    "\n",
    "The result of this operation can be visualized like this: (green corresponds to computing the scaled dot product, and white to a no-op or $-∞$).\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-31_at_7.27.29_PM.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZTy748cWIEC"
   },
   "source": [
    "**TODO**: Implement the sliding_window_attention function in hw3/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CY1gz0fSebQP"
   },
   "outputs": [],
   "source": [
    "from hw3.transformer import sliding_window_attention\n",
    "\n",
    "\n",
    "## test sliding-window attention\n",
    "num_heads = 3\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "embed_dim = 3\n",
    "window_size = 2\n",
    "\n",
    "## test without extra dimension for heads\n",
    "x = torch.arange(seq_len*embed_dim).reshape(seq_len,embed_dim).repeat(batch_size,1).reshape(batch_size, seq_len, -1).float()\n",
    "\n",
    "values, attention = sliding_window_attention(x, x, x,window_size)\n",
    "\n",
    "gt_values = torch.load(os.path.join('test_tensors','values_tensor_0_heads.pt'))\n",
    "\n",
    "\n",
    "test.assertTrue(torch.all(values == gt_values), f'the tensors differ in dims [B,row,col]:{torch.stack(torch.where(values != gt_values),dim=0)}')\n",
    "\n",
    "gt_attention = torch.load(os.path.join('test_tensors','attention_tensor_0_heads.pt'))\n",
    "test.assertTrue(torch.all(attention == gt_attention), f'the tensors differ in dims [B,row,col]:{torch.stack(torch.where(attention != gt_attention),dim=0)}')\n",
    "\n",
    "\n",
    "## test with extra dimension for heads\n",
    "x = torch.arange(seq_len*embed_dim).reshape(seq_len,embed_dim).repeat(batch_size, num_heads, 1).reshape(batch_size, num_heads, seq_len, -1).float()\n",
    "\n",
    "values, attention = sliding_window_attention(x, x, x,window_size)\n",
    "\n",
    "gt_values = torch.load(os.path.join('test_tensors','values_tensor_3_heads.pt'))\n",
    "test.assertTrue(torch.all(values == gt_values), f'the tensors differ in dims [B,num_heads,row,col]:{torch.stack(torch.where(values != gt_values),dim=0)}')\n",
    "\n",
    "\n",
    "gt_attention = torch.load(os.path.join('test_tensors','attention_tensor_3_heads.pt'))\n",
    "test.assertTrue(torch.all(attention == gt_attention), f'the tensors differ in dims [B,num_heads,row,col]:{torch.stack(torch.where(attention != gt_attention),dim=0)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlHiDHhgeS1_"
   },
   "source": [
    "## Multihead Sliding window attention\n",
    "<a id=part3_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen in class, the transformer model uses a Multi-head attention module. We will use the same implementation you've seen in the tutorial, aside from the attention mechanism itslef, which will be swapped with the sliding-window attention you implemented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Insert the call to the sliding-window attention mechanism in the forward of MultiHeadAttention in hw3/transformer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyHo9Oy_Z7SX",
    "tags": []
   },
   "source": [
    "## Sentiment analysis\n",
    "<a id=part3_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6MMtJFRaZCi"
   },
   "source": [
    "We will now go on to tackling the task of sentiment analysis which is the process of analyzing text to determine if the emotional tone of the message is positive or negative (many times a neutral class is also used, but this won't be the case in the data we will be working with).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IMBD hugging face dataset\n",
    "<a id=part3_3_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face is a popular open-source library and platform that provides state-of-the-art tools and resources for natural language processing (NLP) tasks. It has gained immense popularity within the NLP community due to its user-friendly interfaces, powerful pre-trained models, and a vibrant community that actively contributes to its development. \n",
    "\n",
    "Hugging Face provides a wide array of tools and utilities, which we will leverage as well. The Hugging Face Transformers library, built on top of PyTorch and TensorFlow, offers a simple yet powerful API for working with Transformer-based models (such as Distil-BERT). It enables users to easily load, fine-tune, and evaluate models, as well as generate text using these models.\n",
    "\n",
    "Furthermore, Hugging Face offers the Hugging Face Datasets library, which provides access to a vast collection of publicly available datasets for NLP. These datasets can be conveniently downloaded and used for training and evaluation purposes.\n",
    "\n",
    "You are encouraged to visit their site and see other uses: https://huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the dataset using Hugging Face's `datasets` library.\n",
    "\n",
    "Feel free to look around at the full array of datasets that they offer.\n",
    "\n",
    "https://huggingface.co/docs/datasets/index\n",
    "\n",
    "We will load the full training and test sets in addition to a small toy subset of the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/yairdavidson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7559159248284eb9a868027941294bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('imdb', split=['train', 'test', 'train[12480:12520]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "}), Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "}), Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 40\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it returned a list of 3 labeled datasets, the first two of size 25,000, and the third of size 40.\n",
    "We will use these as `train` and `test` datasets for training the model, and the `toy` dataset for a sanity check. \n",
    "These Datasets are wrapped in a `Dataset` class.\n",
    "\n",
    "We now wrap the dataset into a `DatasetDict` class, which contains helpful methods to use for working with the data.   \n",
    "https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wrap it in a DatasetDict to enable methods such as map and format\n",
    "dataset = DatasetDict({'train': dataset[0], 'val': dataset[1], 'toy': dataset[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    toy: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access the datasets in the Dict as we would a dictionary.\n",
    "Let's print a few training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "TRAINING SAMPLE 0:\n",
      "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
      "Label 0: 0\n",
      "\n",
      "\n",
      "TRAINING SAMPLE 1:\n",
      "\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.\n",
      "Label 1: 0\n",
      "\n",
      "\n",
      "TRAINING SAMPLE 2:\n",
      "If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\n",
      "Label 2: 0\n",
      "\n",
      "\n",
      "TRAINING SAMPLE 3:\n",
      "This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\n",
      "Label 3: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'])\n",
    "\n",
    "for i in range(4):\n",
    "    print(f'TRAINING SAMPLE {i}:') \n",
    "    print(dataset['train'][i]['text'])\n",
    "    label = dataset['train'][i]['label']\n",
    "    print(f'Label {i}: {label}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check the label distirbution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative samples in train dataset: 12500\n",
      "positive samples in train dataset: 12500\n",
      "negative samples in val dataset: 12500\n",
      "positive samples in val dataset: 12500\n",
      "negative samples in toy dataset: 20\n",
      "positive samples in toy dataset: 20\n"
     ]
    }
   ],
   "source": [
    "def label_cnt(type):\n",
    "    ds = dataset[type]\n",
    "    size = len(ds)\n",
    "    cnt= 0 \n",
    "    for smp in ds:\n",
    "        cnt += smp['label']\n",
    "    print(f'negative samples in {type} dataset: {size - cnt}')\n",
    "    print(f'positive samples in {type} dataset: {cnt}')\n",
    "    \n",
    "label_cnt('train')\n",
    "label_cnt('val')\n",
    "label_cnt('toy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### __Import the tokenizer for the dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s tokenize the texts into individual word tokens using the tokenizer implementation inherited from the pre-trained model class.  \n",
    "With Hugging Face you will always find a tokenizer associated with each model. If you are not doing research or experiments on tokenizers it’s always preferable to use the standard tokenizers.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create helper functions to tokenize the text. Notice the arguments sent to the tokenizer.  \n",
    "__Padding__ is a strategy for ensuring tensors are rectangular by adding a special padding token to shorter sentences.   \n",
    "On the other hand , sometimes a sequence may be too long for a model to handle. In this case, you’ll need to __truncate__ the sequence to a shorter length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/yairdavidson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-2c94583cc9a60a41.arrow\n",
      "Loading cached processed dataset at /home/yairdavidson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-3c7408eba7eac57c.arrow\n",
      "Loading cached processed dataset at /home/yairdavidson/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-8c7ecfe6aa482bfd.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    dataset_tokenized = dataset.map(tokenize_text, batched=True, batch_size =None)\n",
    "    return dataset_tokenized\n",
    "\n",
    "dataset_tokenized = tokenize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we would like to work with pytorch so we can manually fine-tune\n",
    "dataset_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# no need to parrarelize in this assignment\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### __Setting up the dataloaders and dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now set up the dataloaders for efficient batching and loading of the data.  \n",
    "By now, you are familiar with the Class methods that are needed to create a working Dataloader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(0),\n",
       " 'input_ids': tensor([  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
       "          2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,  2009,\n",
       "          2043,  2009,  2001,  2034,  2207,  1999,  3476,  1012,  1045,  2036,\n",
       "          2657,  2008,  2012,  2034,  2009,  2001,  8243,  2011,  1057,  1012,\n",
       "          1055,  1012,  8205,  2065,  2009,  2412,  2699,  2000,  4607,  2023,\n",
       "          2406,  1010,  3568,  2108,  1037,  5470,  1997,  3152,  2641,  1000,\n",
       "          6801,  1000,  1045,  2428,  2018,  2000,  2156,  2023,  2005,  2870,\n",
       "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n",
       "          5436,  2003,  8857,  2105,  1037,  2402,  4467,  3689,  3076,  2315,\n",
       "         14229,  2040,  4122,  2000,  4553,  2673,  2016,  2064,  2055,  2166,\n",
       "          1012,  1999,  3327,  2016,  4122,  2000,  3579,  2014,  3086,  2015,\n",
       "          2000,  2437,  2070,  4066,  1997,  4516,  2006,  2054,  1996,  2779,\n",
       "         25430, 14728,  2245,  2055,  3056,  2576,  3314,  2107,  2004,  1996,\n",
       "          5148,  2162,  1998,  2679,  3314,  1999,  1996,  2142,  2163,  1012,\n",
       "          1999,  2090,  4851,  8801,  1998,  6623,  7939,  4697,  3619,  1997,\n",
       "          8947,  2055,  2037, 10740,  2006,  4331,  1010,  2016,  2038,  3348,\n",
       "          2007,  2014,  3689,  3836,  1010, 19846,  1010,  1998,  2496,  2273,\n",
       "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2054,\n",
       "          8563,  2033,  2055,  1045,  2572,  8025,  1011,  3756,  2003,  2008,\n",
       "          2871,  2086,  3283,  1010,  2023,  2001,  2641, 26932,  1012,  2428,\n",
       "          1010,  1996,  3348,  1998, 16371, 25469,  5019,  2024,  2261,  1998,\n",
       "          2521,  2090,  1010,  2130,  2059,  2009,  1005,  1055,  2025,  2915,\n",
       "          2066,  2070, 10036,  2135,  2081, 22555,  2080,  1012,  2096,  2026,\n",
       "          2406,  3549,  2568,  2424,  2009, 16880,  1010,  1999,  4507,  3348,\n",
       "          1998, 16371, 25469,  2024,  1037,  2350, 18785,  1999,  4467,  5988,\n",
       "          1012,  2130, 13749,  7849, 24544,  1010, 15835,  2037,  3437,  2000,\n",
       "          2204,  2214,  2879,  2198,  4811,  1010,  2018,  3348,  5019,  1999,\n",
       "          2010,  3152,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
       "          1028,  1045,  2079,  4012,  3549,  2094,  1996, 16587,  2005,  1996,\n",
       "          2755,  2008,  2151,  3348,  3491,  1999,  1996,  2143,  2003,  3491,\n",
       "          2005,  6018,  5682,  2738,  2084,  2074,  2000,  5213,  2111,  1998,\n",
       "          2191,  2769,  2000,  2022,  3491,  1999, 26932, 12370,  1999,  2637,\n",
       "          1012,  1045,  2572,  8025,  1011,  3756,  2003,  1037,  2204,  2143,\n",
       "          2005,  3087,  5782,  2000,  2817,  1996,  6240,  1998, 14629,  1006,\n",
       "          2053, 26136,  3832,  1007,  1997,  4467,  5988,  1012,  2021,  2428,\n",
       "          1010,  2023,  2143,  2987,  1005,  1056,  2031,  2172,  1997,  1037,\n",
       "          5436,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.ds = dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.ds[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(dataset_tokenized['train'])\n",
    "val_dataset = IMDBDataset(dataset_tokenized['val'])\n",
    "toy_dataset = IMDBDataset(dataset_tokenized['toy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl_train,dl_val, dl_toy = [ \n",
    "    DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    "),\n",
    "DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    "),\n",
    "DataLoader(\n",
    "    dataset=toy_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=0\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDSwVcumaY_E",
    "tags": []
   },
   "source": [
    "### Transformer Encoder\n",
    "<a id=part3_3_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDsr_vqWa3V5"
   },
   "source": [
    "The model we will use for the task at hand, is the encoder of the transformer proposed in the seminal paper 'Attention Is All You Need'.\n",
    "\n",
    "The encoder is composed of positional encoding, and then multiple blocks which compute multi-head attention, layer normalization and a feed forward network as described in the diagram below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"imgs/transformer_encoder.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We provided you with implemetations for the positional encoding and the position-wise feed forward MLP in hw3/transformer.py. \n",
    "\n",
    "Feel free to read through the implementations to make sure you understand what they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: To begin with, complete the transformer EncoderLayer in hw3/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hw3.transformer import EncoderLayer\n",
    "# set torch seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "layer = EncoderLayer(embed_dim=16, hidden_dim=16, num_heads=4, window_size=4, dropout=0.1)\n",
    "\n",
    "# load x and y\n",
    "x = torch.load(os.path.join('test_tensors','encoder_layer_input.pt'))\n",
    "y = torch.load(os.path.join('test_tensors','encoder_layer_output.pt'))\n",
    "padding_mask = torch.ones(2, 10)\n",
    "padding_mask[:, 5:] = 0\n",
    "\n",
    "# forward pass\n",
    "out = layer(x, padding_mask)\n",
    "test.assertTrue(torch.allclose(out, y, atol=1e-6), 'output of encoder layer is incorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to classify a sentence using the encoder, we need to somehow summarize the output of the last encoder layer (which will include an output for each token in the tokenized input sentence). \n",
    "\n",
    "There are several options for doing this. We will use the output of the special token [CLS] appended to the beginning of each sentence by the bert tokenizer we are using.\n",
    "\n",
    "Let's see an example of the first tokens in a sentence after tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'rented', 'i', 'am', 'curious', '-', 'yellow', 'from', 'my']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(dataset_tokenized['train'][0]['input_ids'])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**TODO**: Now it's time to put it all together. Complete the implementaion of 'Encoder' in hw3/transformer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hw3.transformer import Encoder\n",
    "\n",
    "# set torch seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "encoder = Encoder(vocab_size=100, embed_dim=16, num_heads=4, num_layers=3, \n",
    "                  hidden_dim=16, max_seq_length=64, window_size=4, dropout=0.1)\n",
    "\n",
    "\n",
    "# load x and y\n",
    "x = torch.load(os.path.join('test_tensors','encoder_input.pt'))\n",
    "y = torch.load(os.path.join('test_tensors','encoder_output.pt'))\n",
    "x = torch.randint(0, 100, (2, 64)).long()\n",
    "\n",
    "padding_mask = torch.ones(2, 64)\n",
    "padding_mask[:, 50:] = 0\n",
    "\n",
    "# forward pass\n",
    "out = encoder(x, padding_mask)\n",
    "test.assertTrue(torch.allclose(out, y, atol=1e-6), 'output of encoder layer is incorrect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the encoder\n",
    "<a id=part3_3_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now proceed to train the model. \n",
    "\n",
    "**TODO**: Complete the implementation of TransformerEncoderTrainer in hw3/training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we will train on a small toy dataset of 40 samples. This will serve as a sanity check to make sure nothing is buggy.\n",
    "\n",
    "**TODO**: choose the hyperparameters in hw3.answers part3_transformer_encoder_hyperparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VWHBI8gm2qo-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_dim': 32, 'num_heads': 4, 'num_layers': 3, 'hidden_dim': 32, 'window_size': 60, 'droupout': 0.1, 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from hw3.answers import part3_transformer_encoder_hyperparams\n",
    "\n",
    "params = part3_transformer_encoder_hyperparams()\n",
    "print(params)\n",
    "embed_dim = params['embed_dim'] \n",
    "num_heads = params['num_heads']\n",
    "num_layers = params['num_layers']\n",
    "hidden_dim = params['hidden_dim']\n",
    "window_size = params['window_size']\n",
    "dropout = params['droupout']\n",
    "lr = params['lr']\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "\n",
    "max_batches_per_epoch = None\n",
    "N_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model = Encoder(vocab_size, embed_dim, num_heads, num_layers, hidden_dim, max_seq_length, window_size, dropout=dropout).to(device)\n",
    "toy_optimizer = optim.Adam(toy_model.parameters(), lr=lr)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit your model\n",
    "import pickle\n",
    "if not os.path.exists('toy_transfomer_encoder.pt'):\n",
    "    # overfit\n",
    "    from hw3.training import TransformerEncoderTrainer\n",
    "    toy_trainer = TransformerEncoderTrainer(toy_model, criterion, toy_optimizer)\n",
    "    # set max batches per epoch\n",
    "    _ = toy_trainer.fit(dl_toy, dl_toy, N_EPOCHS, checkpoints='toy_transfomer_encoder', max_batches=max_batches_per_epoch)\n",
    "\n",
    "    \n",
    "\n",
    "toy_saved_state = torch.load('toy_transfomer_encoder.pt')\n",
    "toy_best_acc = toy_saved_state['best_acc']\n",
    "toy_model.load_state_dict(toy_saved_state['model_state']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.assertTrue(toy_best_acc >= 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You are now ready to train your sentiment analysis classifier!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_batches_per_epoch = 500\n",
    "N_EPOCHS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Encoder(vocab_size, embed_dim, num_heads, num_layers, hidden_dim, max_seq_length, window_size, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit your model\n",
    "import pickle\n",
    "if not os.path.exists('trained_transfomer_encoder.pt'):\n",
    "    from hw3.training import TransformerEncoderTrainer\n",
    "    trainer = TransformerEncoderTrainer(model, criterion, optimizer)\n",
    "    # set max batches per epoch\n",
    "    _ = trainer.fit(dl_train, dl_val, N_EPOCHS, checkpoints='trained_transfomer_encoder', max_batches=max_batches_per_epoch)\n",
    "    \n",
    "\n",
    "saved_state = torch.load('trained_transfomer_encoder.pt')\n",
    "best_acc = saved_state['best_acc']\n",
    "model.load_state_dict(saved_state['model_state']) \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.assertTrue(best_acc >= 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the follwing cells to see an example of the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22490])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_index = torch.randint(len(dataset_tokenized['val']), (1,))\n",
    "rand_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I just saw \"A Tale of Two Sisters\" last night and really enjoyed it. I\\'ve been a big fan of Asian horror films recently and think that this is a strong entry from South Korea. There aren\\'t many jump out at you scares as in the usual American horror film, but the director does maintain the off-kilter and foreboding mood very well, especially in the awkward character interactions with each other. Most of the scares are more conceptual and plays on everyone\\'s \"there\\'s something under the bed\" fears from when they were a child, but in this case, it\\'s the closet and the sink. I also liked how the director was able to capture just how dysfunctional this household is through scenes such as the first dinner that the characters have together. He\\'s also good at revealing people\\'s inner life and fragility through simple scenes such as the stepmother wiping off her make-up in the mirror or her sitting in front of the flickering TV. I think this film is mainly an exploration of guilt and the consequences of living with that guilt hanging over you.<br /><br />MAJOR SPOILERS AHEAD (DO NOT READ ANY FURTHER IF YOU DO NOT WANT THE TWISTS OF THE MOVIE REVEALED) I was following the story pretty well, but did start getting confused during the bag dragging part. However, I think the flashback at the end definitely tied everything together. The film is very much like \"The Machinist\" in the way two of the character\\'s joint guilt eventually leads to mental breakdowns and delusions.<br /><br />Here\\'s my interpretation of the film. The Su-Yeon that we see after the girls supposed return to the house is either the delusion of Su-Mi or the actual ghost of Su-Yeon that only Su-Mi can see and interact with. The initial stepmother that we see is, in my opinion a delusion of Su-Mi. There is a real stepmother, however, and she first appears in the film when she\\'s wearing the gray pantsuit. I believe it\\'s the real stepmother that the father is talking to on the phone throughout the first part of the movie and she doesn\\'t appear until he pick her up and brings her to the house. The stepmother before that point is imagined by Su-Mi (perhaps part of her split personality?) That explains the bizarre dinner party sequence when the stepmother\\'s brother looks at her like she\\'s crazy and doesn\\'t remember anything that she recounts. I think it was Su-Mi acting out her stepmother part of her split personality. The film shows this later in the bag dragging scene and scenes such as the stepmother wiping her make-up in the mirror, which is revealed later to actually be Su-Mi wiping her make-up in the mirror.<br /><br />I think the ghosts in the house aren\\'t entirely imagined by Su-Mi, and are either of Su-Yeon or the mother or both. In the final flashback, it is revealed the Su-Yeon was wearing the green dress and had the hairpin in her hair when she died. This is the green dress that they showed before on the ghost sitting at the dining room table while the stepmother was looking under the sink. Also, it\\'s the hairpin that Su-Yeon was wearing in the flashback that appears on the floor when the stepmother is looking under the sink.<br /><br />The real stepmother, in the end, gets punished by the ghost of Su-Yeon who comes for in a scene a little bit like The Ring. After that, the flashback scene ties it all together in terms of how both the stepmother was mainly responsible for her death, while Su-Mi unintentionally played a supporting role.<br /><br />I wonder if the \"mother\" that Su-Yeon sees when she goes up to her room to cry, in the flashback, is a ghost already. Perhaps by that point the mother had already killed herself in the closet. That\\'s left ambiguous.<br /><br />Other things that are suggested, but not clearly explained in the film is that it seems like the stepmother, at some point, was a nurse, perhaps taking care of the mother and somehow may have contributed to her death too. It\\'s not clear when her relationship with the father began and whether it caused the mother to kill herself. It\\'s also suggested that the mother had mental issues too, requiring a nurse. The stepmother alludes to this when she tells Su-Mi, you\\'re beginning to take after your mother. I don\\'t think she meant just physically.<br /><br />Also, if we accept that the initial stepmother that we see is actually Su-Mi, then there\\'s the suggestion of incest too, since the father sleeps with her. Is that why Su-Mi freaks out and shouts, \"Don\\'t touch me\" each time the father reaches for her in a later scenes? Is that the \"filthy things that you\\'ve done\" that she alludes to in a later conversation with the father? This film is interesting in it\\'s capacity for different interpretations. A few of the scenes, however, were kind of derivative, such as the woman in the black crawling around scene, which reminded me of the herky-jerky movements of Kayako in the Ju-On/The Grudge films. Also, the final scene where the stepmother finally gets her just desserts is reminiscent of The Ring. Furthermore, just the idea that some characters may be ghosts is taken from \"The Sixth Sense\" or \"The Others\".<br /><br />Overall, I enjoyed it, however, and it will be interesting to see how the Hollywood remake (that\\'s already in production) turns out. I have to be honest, I liked both \"The Ring\" and \"The Grudge\", so I\\'m not one of those snooty types who insist that remakes can\\'t be good too. One remake that I\\'m really excited about is \"Dark Water\" coming out this summer. I haven\\'t seen the original Japanese version yet, but both films are definitely on my to-see list.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset['val'][rand_index]\n",
    "sample['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label torch.Size([1])\n",
      "attention_mask torch.Size([1, 512])\n",
      "label: tensor([1], device='cuda:0'), prediction: tensor([1.], device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "tokenized_sample = dataset_tokenized['val'][rand_index]\n",
    "tokenized_sample\n",
    "input_ids = tokenized_sample['input_ids'].to(device)\n",
    "label = tokenized_sample['label'].to(device)\n",
    "attention_mask = tokenized_sample['attention_mask'].to(float).to(device)\n",
    "\n",
    "print('label', label.shape)\n",
    "print('attention_mask', attention_mask.shape)\n",
    "prediction = model.predict(input_ids, attention_mask).squeeze(0)\n",
    "\n",
    "print('label: {}, prediction: {}'.format(label, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part you wil see how to fine-tune a pretrained model for the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw3.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill your answers in hw3.answers.part3_q1 and hw3.answers.part3_q2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain why stacking encoder layers that use the sliding-window attention results in a broader context in the final layer.\n",
    "Hint: Think what happens when stacking CNN layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Your answer:**\n",
       "The receptive field of the layers grow with the depth, \n",
       "resulting in a larger context for each token.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw3.answers.part3_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propose a variation of the attention pattern such that the computational complexity stays similar to that of the sliding-window attention O(nw), but the attention is computed on a more global context.\n",
    "Note: There is no single correct answer to this, feel free to read the paper that proposed the sliding-window. Any solution that makes sense will be considered correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Your answer:**\n",
       "Options:\n",
       "sparse attention - attend to a subset of the input tokens but not neccecerily those close to the query.\n",
       "dilated sliding window - attend to tokens in a sliding window with 'holes' in it.\n",
       "sliding window with global attention - attend to tokens in a sliding window \n",
       "with global attention for specific queries (for example for the [CLS] token).\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw3.answers.part3_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
